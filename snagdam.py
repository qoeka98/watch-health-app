import streamlit as st
from huggingface_hub import InferenceClient
import re

# âœ… ì‚¬ìš©ì ì…ë ¥ ì •ì œ
def clean_input(text):
    text = re.sub(r"\b(í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´ ì¤˜|ë§í•´ ì¤˜)\b", "", text, flags=re.IGNORECASE)
    return text.strip()

# âœ… ê±´ê°• ê´€ë ¨ í‚¤ì›Œë“œ íŒë³„
def is_health_related(text):
    health_keywords = [
        "ê±´ê°•", "ì˜í•™", "ì˜ë£Œ", "ì•½í•™", "í•œì˜í•™", "ë‹¹ë‡¨", "ë¹„ë§Œ", "ê³ ì§€í˜ˆì¦", "ê³ í˜ˆì••",
        "ìš´ë™", "ì˜ì–‘", "ì½œë ˆìŠ¤í…Œë¡¤", "í˜ˆì••", "í˜ˆë‹¹", "ì²´ì¤‘", "ì‹¬ì¥", "ì‹ ì¥", "ì‹ìŠµê´€",
        "í˜ˆì•¡ ê²€ì‚¬", "ë‹¹ë‡¨ë³‘", "ì €í˜ˆì••", "ì²´ì§ˆëŸ‰", "ì½œë ˆìŠ¤í…Œë¡¤ ìˆ˜ì¹˜",
        "ì•”", "ìœ„ì•”", "ê°„ì•”", "ëŒ€ì¥ì•”", "ì‹¬ì¥ë³‘", "ë‡Œì¡¸ì¤‘", "ì‹¬ê·¼ê²½ìƒ‰", "í˜‘ì‹¬ì¦", 
        "ì¹˜ë§¤", "íŒŒí‚¨ìŠ¨ë³‘", "ìš°ìš¸ì¦", "ë¶ˆì•ˆì¥ì• ", "ìŠ¤íŠ¸ë ˆìŠ¤", "ì•Œì¸ í•˜ì´ë¨¸", "ì²œì‹",
        "ê°„ê²½í™”", "ì‹ ë¶€ì „", "ìœ„ì—¼", "ì¥ì—¼", "ì†Œí™”ë¶ˆëŸ‰", "ê°‘ìƒì„ ", "ë¥˜ë§ˆí‹°ìŠ¤", "ê´€ì ˆì—¼",
        "ë‹¤ì´ì–´íŠ¸", "ì‹ì´ìš”ë²•", "ì˜ì–‘ì†Œ", "ì¹¼ìŠ˜", "ì² ë¶„", "ë‹¨ë°±ì§ˆ", "ë¹„íƒ€ë¯¼", "ë¯¸ë„¤ë„",
        "ì„­ì·¨ëŸ‰", "ì¹¼ë¡œë¦¬", "ì €ì—¼ì‹", "ê³ ë‹¨ë°±", "ì±„ì‹", "ë¹„ê±´", "ê°„í—ì  ë‹¨ì‹",
        "ë©´ì—­ë ¥", "ìˆ˜ë©´", "ê±´ê°•ê²€ì§„", "ì˜ˆë°©ì ‘ì¢…", "ìš´ë™ë²•", "ê±´ê°•ê°œì„ ", "ì‹ë‹¨",
        "ìœ ì‚°ì†Œ ìš´ë™", "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "ëª…ìƒ", "í˜¸í¡ë²•",
        "í˜ˆë‹¹ ìˆ˜ì¹˜", "í˜ˆì•• ìˆ˜ì¹˜", "ì²´ì§€ë°©ë¥ ", "BMI", "ì½œë ˆìŠ¤í…Œë¡¤ ê²€ì‚¬", "ê°„ ìˆ˜ì¹˜",
        "ì‹ ì¥ ê¸°ëŠ¥ ê²€ì‚¬", "ì‹¬ì „ë„", "ë¹ˆí˜ˆ ê²€ì‚¬", "ì§ˆë³‘"
    ]
    return any(keyword in text for keyword in health_keywords)

# âœ… ì‘ë‹µì—ì„œ ì§ˆë¬¸ ì œê±° ë° ì²« ì¤„ ì œê±°
def filter_ai_response(response, user_input):
    response = response.replace(user_input, "").strip()
    response_lines = response.split("\n")
    if len(response_lines) > 1:
        response = "\n".join(response_lines[1:]).strip()
    return response

# âœ… Hugging Face API í† í°
def get_huggingface_token():
    return st.secrets.get("HUGGINGFACE_API_TOKEN")

# âœ… ì±—ë´‡ ì‹¤í–‰
def run_snagdam():
    st.title("ğŸ’¬ ê±´ê°• ìƒë‹´ ì±—ë´‡")
    st.info(
        '''ê±´ê°• ì˜ˆì¸¡ì„ ë°”íƒ•ìœ¼ë¡œ ê±´ê°• ìƒë‹´ì„ ì§„í–‰í•´ë³´ì„¸ìš”! ğŸ©º

        **ì˜ˆì‹œ ì§ˆë¬¸:**  
        - ê±´ê°• ìƒíƒœë¥¼ ê°œì„ í•˜ë ¤ë©´ ì–´ë–¤ ìš´ë™ì´ ì¢‹ì„ê¹Œìš”?  
        - ì‹ë‹¨ì„ ì–´ë–»ê²Œ ì¡°ì ˆí•˜ë©´ ì¢‹ì„ê¹Œìš”?  
        - íŠ¹ì • ì§ˆë³‘ì˜ ìœ„í—˜ì„ ë‚®ì¶”ê¸° ìœ„í•œ ìƒí™œ ìŠµê´€ì€?  
        - í˜ˆì••ì„ ë‚®ì¶”ëŠ” ë°©ë²•ì—ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?  
        '''
    )

    token = get_huggingface_token()
    client = InferenceClient(
        model="HuggingFaceH4/zephyr-7b-beta",
        api_key=token
    )

    # âœ… ì´ˆê¸° ì„¸ì…˜ ë©”ì‹œì§€
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # âœ… ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # âœ… ì‚¬ìš©ì ì…ë ¥
    chat = st.chat_input("ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")

    if chat:
        clean_chat = clean_input(chat)

        if not is_health_related(clean_chat):
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ë§Œ ìƒë‹´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:
            # âœ… system prompt ê°œì„ 
            system_prompt = (
                "ë‹¹ì‹ ì€ ê±´ê°• ì „ë¬¸ ìƒë‹´ AIì…ë‹ˆë‹¤. "
                "ì§ˆë³‘ ì˜ˆë°©, ìš´ë™, ì‹ì´ìš”ë²• ë“± ê±´ê°•ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. "
                "ë„ˆë¬´ ë°˜ë³µì ì¸ í‘œí˜„ì€ í”¼í•˜ê³ , ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                " ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. "
                "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ 300ìë„˜ì§€ ì•Šê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. "
                "ë¬¸ì¥ì€ ë°˜ë“œì‹œ ì™„ê²°ë˜ë„ë¡ ëë§ºì–´ì£¼ì„¸ìš”."
                "ë¬¸ì¥ì€ ë°˜ë“œì‹œ ê³µì†íˆ ë¬¸ì¥ì„ ì™„ì„±í•´ì£¼ì„¸ìš”."
            )

            full_prompt = system_prompt + "\n\nì§ˆë¬¸: " + clean_chat

            # âœ… ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            st.session_state.messages.append({"role": "user", "content": clean_chat})
            with st.chat_message("user"):
                st.markdown(clean_chat)

            with st.spinner("AIê°€ ì‘ë‹µì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    raw_response = client.text_generation(
                        prompt=full_prompt,
                        max_new_tokens=300
                    )
                    response = filter_ai_response(raw_response, clean_chat)

                    # âœ… ë°˜ë³µì ì¸ ì´ìƒ ì‘ë‹µ í•„í„°ë§
                    if any(keyword in response for keyword in ["ìŠ¤ì¿¨ì§€ì–´", "ìŠ¤íƒ­ìŠ¤íƒ€ì´ì €", "ê°€ìŠ´ê³¼ í—ˆë²…ì§€", "ê°™ì€ ìš´ë™"]):
                        response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”. ë‹¤ì‹œ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”."

                except Exception as e:
                    response = f"âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

        # âœ… AI ì‘ë‹µ ì €ì¥ ë° ì¶œë ¥
        st.session_state.messages.append({"role": "assistant", "content": response})
        with st.chat_message("assistant"):
            st.markdown(response)
