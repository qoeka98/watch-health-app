import streamlit as st
from transformers import pipeline

# âœ… ë¡œì»¬ì—ì„œ ë°”ë¡œ ì‹¤í–‰ë˜ëŠ” text-generation íŒŒì´í”„ë¼ì¸ (Mistral ê¸°ë°˜)
@st.cache_resource
def load_model():
    return pipeline("text-generation", model="mistralai/Mistral-7B-Instruct-v0.1")

generator = load_model()

def run_snagdam():
    st.title("ğŸ’¬ ê±´ê°• ìƒë‹´ ì±—ë´‡ (ë¡œì»¬ ì‹¤í–‰)")

    st.info("ì´ ëª¨ë¸ì€ í† í° ì—†ì´ ë¡œì»¬ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤. ì²« ë¡œë”©ì—ëŠ” ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    user_input = st.chat_input("ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.spinner("AIê°€ ì‘ë‹µ ì¤‘ì…ë‹ˆë‹¤..."):
            prompt = f"[INST] ê±´ê°• ì „ë¬¸ê°€ë¡œì„œ ë‹µí•´ì¤˜: {user_input} [/INST]"
            output = generator(prompt, max_new_tokens=256, do_sample=True)[0]['generated_text']

        response = output.replace(prompt, "").strip()
        st.session_state.messages.append({"role": "assistant", "content": response})
        with st.chat_message("assistant"):
            st.markdown(response)

def main():
    run_snagdam()
