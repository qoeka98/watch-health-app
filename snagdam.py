import streamlit as st
from huggingface_hub import InferenceClient
import re

def clean_input(text):
    """ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°"""
    text = re.sub(r"\b(í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´ ì¤˜|ë§í•´ ì¤˜)\b", "", text, flags=re.IGNORECASE)
    return text.strip()

def is_health_related(text):
    """ê±´ê°• ê´€ë ¨ í‚¤ì›Œë“œ íƒìƒ‰"""
    health_keywords = [
        "ê±´ê°•", "ì˜í•™", "ì˜ë£Œ", "ì•½í•™", "í•œì˜í•™", "ë‹¹ë‡¨", "ë¹„ë§Œ", "ê³ ì§€í˜ˆì¦", "ê³ í˜ˆì••",
        "ìš´ë™", "ì˜ì–‘", "ì½œë ˆìŠ¤í…Œë¡¤", "í˜ˆì••", "í˜ˆë‹¹", "ì²´ì¤‘", "ì‹¬ì¥", "ì‹ ì¥", "ì‹ìŠµê´€",
        "í˜ˆì•¡ ê²€ì‚¬", "ë‹¹ë‡¨ë³‘", "ì €í˜ˆì••", "ì²´ì§ˆëŸ‰", "ì½œë ˆìŠ¤í…Œë¡¤ ìˆ˜ì¹˜", "ì•”", "íì•”", "ê°„ì•”",
        "ìœ„ì•”", "ëŒ€ì¥ì•”", "ì‹¬ì¥ë³‘", "ë‡Œì¡¸ì¤‘", "ì‹¬ê·¼ê²½ìƒ‰", "í˜‘ì‹¬ì¦", "ì¹˜ë§¤", "ìš°ìš¸ì¦",
        "ìŠ¤íŠ¸ë ˆìŠ¤", "ì²œì‹", "ê°„ê²½í™”", "ì‹ ë¶€ì „", "ìœ„ì—¼", "ì¥ì—¼", "ì†Œí™”ë¶ˆëŸ‰", "ê°‘ìƒì„ ",
        "ë¥˜ë§ˆí‹°ìŠ¤", "ê´€ì ˆì—¼", "ì‹ì´ìš”ë²•", "ì˜ì–‘ì†Œ", "ì¹¼ìŠ˜", "ì² ë¶„", "ë‹¨ë°±ì§ˆ", "ë¹„íƒ€ë¯¼",
        "ì„­ì·¨ëŸ‰", "ì¹¼ë¡œë¦¬", "ì €ì—¼ì‹", "ê³ ë‹¨ë°±", "ì±„ì‹", "ë¹„ê±´", "í‚¤í† ì œë‹‰", "ê°„í—ì  ë‹¨ì‹",
        "ë©´ì—­ë ¥", "ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬", "ìˆ˜ë©´", "ë¶ˆë©´ì¦", "ê±´ê°•ê²€ì§„", "ì˜ˆë°©ì ‘ì¢…", "ìš´ë™ë²•",
        "ê·¼ë ¥ ìš´ë™", "ìœ ì‚°ì†Œ ìš´ë™", "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "ëª…ìƒ", "í˜¸í¡ë²•", "BMI", 
        "ì‹¬ì „ë„", "í˜ˆì•¡í˜•", "ë¹ˆí˜ˆ ê²€ì‚¬", "ê±´ê°•ìƒíƒœ", "ì§ˆë³‘"
    ]
    return any(keyword in text for keyword in health_keywords)

def filter_ai_response(response, user_input):
    """AI ì‘ë‹µì—ì„œ ë°˜ë³µë˜ëŠ” ì§ˆë¬¸ ì œê±°"""
    response = response.replace(user_input, "").strip()
    response_lines = response.split("\n")
    if len(response_lines) > 1:
        response = "\n".join(response_lines[1:]).strip()
    return response

def get_huggingface_token():
    # ğŸ” ë¡œì»¬ ê°œë°œ ì‹œ ì§ì ‘ ì…ë ¥ / ë°°í¬ ì‹œ secrets.toml ì‚¬ìš©
    return "hf_your_token_here"  # ì—¬ê¸°ì— ë³¸ì¸ì˜ HF API í† í° ì…ë ¥

def run_snagdam():
    st.title("ğŸ’¬ ê±´ê°• ìƒë‹´ ì±—ë´‡")

    st.info(
        '''ê±´ê°• ì˜ˆì¸¡ì„ ë°”íƒ•ìœ¼ë¡œ ê±´ê°• ìƒë‹´ì„ ì§„í–‰í•´ë³´ì„¸ìš”! ğŸ©º  

        **ì˜ˆì‹œ ì§ˆë¬¸:**  
        - ê±´ê°• ìƒíƒœë¥¼ ê°œì„ í•˜ë ¤ë©´ ì–´ë–¤ ìš´ë™ì´ ì¢‹ì„ê¹Œìš”?  
        - ì‹ë‹¨ì„ ì–´ë–»ê²Œ ì¡°ì ˆí•˜ë©´ ì¢‹ì„ê¹Œìš”?  
        - íŠ¹ì • ì§ˆë³‘ì˜ ìœ„í—˜ì„ ë‚®ì¶”ê¸° ìœ„í•œ ìƒí™œ ìŠµê´€ì€?  
        - ì •ê¸° ê±´ê°• ê²€ì§„ì€ ì–¼ë§ˆë‚˜ ìì£¼ ë°›ì•„ì•¼ í•˜ë‚˜ìš”?  
        - í˜ˆì••ì„ ë‚®ì¶”ëŠ” ë°©ë²•ì—ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?  
        '''
    )

    token = get_huggingface_token()

    # âœ… Mistral ëª¨ë¸ë¡œ ì„¤ì •
    client = InferenceClient(
        model="mistralai/Mistral-7B-Instruct-v0.2",
        token=token
    )

    # âœ… ì´ˆê¸° ë©”ì‹œì§€ ì„¸íŒ…
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "user", "content": "ê±´ê°• ìƒë‹´ì„ ì‹œì‘í•´ì£¼ì„¸ìš”"}
        ]

    for message in st.session_state.messages:
        with st.chat_message("user" if message["role"] == "user" else "assistant"):
            st.markdown(message["content"])

    # âœ… ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    chat = st.chat_input("ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")

    if chat:
        clean_chat = clean_input(chat)

        if not is_health_related(clean_chat):
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ë§Œ ë‹µë³€í•  ìˆ˜ ìˆì–´ìš”."
        else:
            # âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—­í•  ì§€ì •)
            system_prompt = (
                "ë„ˆëŠ” ê±´ê°• ì „ë¬¸ê°€ì•¼. ë‹¹ë‡¨, ê³ í˜ˆì••, ê³ ì§€í˜ˆì¦, ë¹„ë§Œ, ë‹¤ì´ì–´íŠ¸ ë“± "
                "ì˜í•™ì  ì •ë³´ë¥¼ ì‹ ì¤‘í•˜ê³  ì •í™•í•˜ê²Œ ì„¤ëª…í•´ì¤˜ì•¼ í•´."
            )

            # âœ… ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° UI ì¶œë ¥
            st.session_state.messages.append({"role": "user", "content": clean_chat})
            with st.chat_message("user"):
                st.markdown(clean_chat)

            # âœ… AI ì‘ë‹µ
            with st.spinner("AIê°€ ì‘ë‹µì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                full_prompt = f"[INST] {system_prompt}\n\n{clean_chat} [/INST]"

                response = client.text_generation(
                    prompt=full_prompt,
                    max_new_tokens=512,
                    temperature=0.7
                )

                response = filter_ai_response(response, clean_chat)

        # âœ… ì‘ë‹µ ì €ì¥ ë° ì¶œë ¥
        st.session_state.messages.append({"role": "assistant", "content": response})
        with st.chat_message("assistant"):
            st.markdown(response)

# âœ… ë©”ì¸ í•¨ìˆ˜ ì§„ì…ì 
def main():
    run_snagdam()


