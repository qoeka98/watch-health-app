import streamlit as st
from huggingface_hub import InferenceClient
import re

def clean_input(text):
    """ë¶ˆí•„ìš”í•œ ë‹¨ì–´(í•´ì¤˜, ì•Œë ¤ì¤˜ ë“±)ë¥¼ ì œê±°í•œ ì‚¬ìš©ì ì…ë ¥ì„ ë°˜í™˜"""
    text = re.sub(r"\b(í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´ ì¤˜|ë§í•´ ì¤˜)\b", "", text, flags=re.IGNORECASE)
    return text.strip()

def is_health_related(text):
    """ì…ë ¥ëœ ì§ˆë¬¸ì´ ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ íŒë³„"""
    health_keywords = [
        # âœ… ê¸°ë³¸ ê±´ê°• ê´€ë ¨ í‚¤ì›Œë“œ
        "ê±´ê°•", "ì˜í•™", "ì˜ë£Œ", "ì•½í•™", "í•œì˜í•™", "ë‹¹ë‡¨", "ë¹„ë§Œ", "ê³ ì§€í˜ˆì¦", "ê³ í˜ˆì••",
        "ìš´ë™", "ì˜ì–‘", "ì½œë ˆìŠ¤í…Œë¡¤", "í˜ˆì••", "í˜ˆë‹¹", "ì²´ì¤‘", "ì‹¬ì¥", "ì‹ ì¥", "ì‹ìŠµê´€",
        "í˜ˆì•¡ ê²€ì‚¬", "ë‹¹ë‡¨ë³‘", "ì €í˜ˆì••", "ì²´ì§ˆëŸ‰", "ì½œë ˆìŠ¤í…Œë¡¤ ìˆ˜ì¹˜",
        
        # âœ… ì¶”ê°€ëœ ì§ˆí™˜ ê´€ë ¨ í‚¤ì›Œë“œ
        "ì•”", "íì•”", "ê°„ì•”", "ìœ„ì•”", "ëŒ€ì¥ì•”", "ì‹¬ì¥ë³‘", "ë‡Œì¡¸ì¤‘", "ì‹¬ê·¼ê²½ìƒ‰", "í˜‘ì‹¬ì¦", 
        "ì¹˜ë§¤", "íŒŒí‚¨ìŠ¨ë³‘", "ìš°ìš¸ì¦", "ë¶ˆì•ˆì¥ì• ", "ìŠ¤íŠ¸ë ˆìŠ¤", "ì•Œì¸ í•˜ì´ë¨¸", "ì²œì‹", "íì§ˆí™˜",
        "ê°„ê²½í™”", "ì‹ ë¶€ì „", "ìœ„ì—¼", "ì¥ì—¼", "ì†Œí™”ë¶ˆëŸ‰", "ê°‘ìƒì„ ", "ë¥˜ë§ˆí‹°ìŠ¤", "ê´€ì ˆì—¼",

        # âœ… ì˜ì–‘ ë° ë‹¤ì´ì–´íŠ¸ ê´€ë ¨ í‚¤ì›Œë“œ
        "ë‹¤ì´ì–´íŠ¸", "ì‹ì´ìš”ë²•", "ì˜ì–‘ì†Œ", "ì¹¼ìŠ˜", "ì² ë¶„", "ë‹¨ë°±ì§ˆ", "ë¹„íƒ€ë¯¼", "ë¯¸ë„¤ë„",
        "ì„­ì·¨ëŸ‰", "ì¹¼ë¡œë¦¬", "ì €ì—¼ì‹", "ê³ ë‹¨ë°±", "ì±„ì‹", "ë¹„ê±´", "í‚¤í† ì œë‹‰", "ê°„í—ì  ë‹¨ì‹",
        
        # âœ… ìƒí™œ ìŠµê´€ ë° ì˜ˆë°© ê´€ë ¨ í‚¤ì›Œë“œ
        "ë©´ì—­ë ¥", "ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬", "ìˆ˜ë©´", "ë¶ˆë©´ì¦", "ê±´ê°•ê²€ì§„", "ì˜ˆë°©ì ‘ì¢…", "ìš´ë™ë²•",
        "ê·¼ë ¥ ìš´ë™", "ìœ ì‚°ì†Œ ìš´ë™", "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "ëª…ìƒ", "í˜¸í¡ë²•",

        # âœ… íŠ¹ì • ê²€ì‚¬ ë° ìˆ˜ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œ
        "í˜ˆë‹¹ ìˆ˜ì¹˜", "í˜ˆì•• ìˆ˜ì¹˜", "ì²´ì§€ë°©ë¥ ", "BMI", "ì½œë ˆìŠ¤í…Œë¡¤ ê²€ì‚¬", "ê°„ ìˆ˜ì¹˜",
        "ì‹ ì¥ ê¸°ëŠ¥ ê²€ì‚¬", "ì‹¬ì „ë„", "í˜ˆì•¡í˜•", "ë¹ˆí˜ˆ ê²€ì‚¬"
    ]
    return any(keyword in text for keyword in health_keywords)

def filter_ai_response(response, user_input):
    """AI ì‘ë‹µì—ì„œ ì²« ì¤„(ì§ˆë¬¸ì„ ë°˜ë³µí•œ ë¶€ë¶„)ì„ ì œê±°"""
    response = response.replace(user_input, "").strip()  # ì‚¬ìš©ì ì§ˆë¬¸ ì œê±°
    response_lines = response.split("\n")  # ì‘ë‹µì„ ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• 
    if len(response_lines) > 1:
        response = "\n".join(response_lines[1:]).strip()  # âœ… ì²« ì¤„ ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ ë°˜í™˜
    return response
def get_huggingface_token():
    
    token=st.secrets.get("HUGGINGFACE_API_TOKEN")
    return token

def run_snagdam():
    st.title("ğŸ’¬ ê±´ê°• ìƒë‹´ ì±—ë´‡")
    st.info('''ê±´ê°•ì˜ˆì¸¡ì„ ë°”íƒ•ìœ¼ë¡œ ê±´ê°• ìƒë‹´ì„ ì§„í–‰í•´ë³´ì„¸ìš”! ''')
    token=get_huggingface_token()
    # âœ… Hugging Face Inference API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    client = InferenceClient(
        
        model="google/gemma-2-9b-it", 
        api_key=token
    )

    # âœ… ëŒ€í™” ê¸°ë¡ ì €ì¥ (ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€)
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "user", "content": "ê±´ê°• ìƒë‹´ì„ ì§„í–‰í•´ì£¼ì„¸ìš”"},{
                "role":"system","content": "ë‹¹ì‹ ì€ ê±´ê°•, ì˜í•™ , ì˜ë£Œ ì „ë¬¸ê°€ AI ì…ë‹ˆë‹¤"
            }
        ]

    # âœ… ê¸°ì¡´ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message("user" if message["role"] == "user" else "assistant"):
            st.markdown(message["content"])

    # âœ… ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    chat = st.chat_input("ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")

    if chat:
        clean_chat = clean_input(chat)  # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°

        if not is_health_related(clean_chat):
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ê±´ê°• ê´€ë ¨ ìƒë‹´ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        else:
            # âœ… AIê°€ ì‚¬ìš©ì ì…ë ¥ì„ í¬í•¨í•˜ì§€ ì•Šë„ë¡ í”„ë¡¬í”„íŠ¸ ê°•í™”
            system_prompt = (
                "ë„ˆëŠ” ê±´ê°•, ì˜í•™, ì˜ë£Œ, ì•½í•™, í•œì˜í•™, ë‹¹ë‡¨ë³‘, ë¹„ë§Œ, ê³ ì§€í˜ˆì¦, ê³ í˜ˆì•• ì „ë¬¸ê°€ AIì•¼. "
                "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , **ì ˆëŒ€** ì‚¬ìš©ì ì§ˆë¬¸ì„ ë‹¤ì‹œ í¬í•¨í•˜ì§€ ë§ê³  ë°”ë¡œ ë‹µë³€ë§Œ ì œê³µí•´. "
                "ì¶”ê°€ì ì¸ ë¬¸ì¥ì€ ìƒì„±í•˜ì§€ ì•Šê³ , ì˜¤ì§ í•µì‹¬ ì •ë³´ë§Œ ì „ë‹¬í•´."
            )

            # âœ… ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ & UIì— í‘œì‹œ
            st.session_state.messages.append({"role": "user", "content": clean_chat})
            with st.chat_message("user"):
                st.markdown(clean_chat)

            # âœ… AI ì‘ë‹µ ìš”ì²­ (Gemma ëª¨ë¸ ì‚¬ìš©)
            full_prompt = system_prompt + "\n\n" + clean_chat

            response = client.text_generation(
                prompt=full_prompt,
                max_new_tokens=250
            )

            # âœ… ì‘ë‹µì—ì„œ ì²« ì¤„ ì œê±°
            response = filter_ai_response(response, clean_chat)

        # âœ… AI ì‘ë‹µ ì €ì¥ & UI í‘œì‹œ
        st.session_state.messages.append({"role": "assistant", "content": response})
        with st.chat_message("assistant"):
            st.markdown(response)
