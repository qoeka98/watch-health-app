import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import re

# âœ… 1. ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ 1íšŒë§Œ)
@st.cache_resource
def load_model():
    model_name = "beomi/KoAlpaca-Polyglot-12.8B"  # ë˜ëŠ” 'skt/kogpt2-base-v2' ë“±
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return tokenizer, model

tokenizer, model = load_model()

# âœ… ì‚¬ìš©ì ì…ë ¥ ì •ì œ
def clean_input(text):
    text = re.sub(r"\b(í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´ ì¤˜|ë§í•´ ì¤˜)\b", "", text, flags=re.IGNORECASE)
    return text.strip()

# âœ… ê±´ê°• í‚¤ì›Œë“œ í•„í„°
def is_health_related(text):
    return any(k in text for k in ["ê±´ê°•", "ìš´ë™", "ì§ˆë³‘", "ì˜ì–‘", "ì‹ì´ìš”ë²•", "í˜ˆì••", "ë¹„ë§Œ", "ë‹¹ë‡¨", "ìˆ˜ë©´"])

# âœ… Streamlit UI
st.title("ğŸ§  ê±´ê°• ìƒë‹´ ì±—ë´‡ (KoLLM ë¡œì»¬ ì‹¤í–‰)")
st.info("í•œêµ­ì–´ LLMì„ ë¡œì»¬ì—ì„œ ì§ì ‘ ì‹¤í–‰í•œ ê±´ê°• ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.")

if "messages" not in st.session_state:
    st.session_state.messages = []

# âœ… ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_input = st.chat_input("ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")

if user_input:
    clean_chat = clean_input(user_input)
    st.session_state.messages.append({"role": "user", "content": clean_chat})
    with st.chat_message("user"):
        st.markdown(clean_chat)

    if not is_health_related(clean_chat):
        ai_response = "â—ì£„ì†¡í•©ë‹ˆë‹¤. ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ë§Œ ìƒë‹´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    else:
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = (
            "ë‹¹ì‹ ì€ ê±´ê°• ì „ë¬¸ ìƒë‹´ AIì…ë‹ˆë‹¤. "
            "ìš´ë™, ì‹ë‹¨, ì§ˆë³‘ ì˜ˆë°©ì— ëŒ€í•´ ì •í™•í•˜ê³  ê³µì†í•˜ê²Œ 300ì ì´ë‚´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n\n"
            f"ì§ˆë¬¸: {clean_chat}\n\në‹µë³€:"
        )

        # í† í¬ë‚˜ì´ì§• ë° ìƒì„±
        input_ids = tokenizer.encode(system_prompt, return_tensors="pt")
        with torch.no_grad():
            output = model.generate(
                input_ids=input_ids,
                max_length=300,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        ai_response = tokenizer.decode(output[0], skip_special_tokens=True)
        ai_response = ai_response.replace(system_prompt, "").strip()

    st.session_state.messages.append({"role": "assistant", "content": ai_response})
    with st.chat_message("assistant"):
        st.markdown(ai_response)
